{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os, sys\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.pyplot import cm\n",
    "import re\n",
    "from datetime import time\n",
    "from datetime import datetime\n",
    "\n",
    "from scipy.io import readsav\n",
    "from scipy import signal\n",
    "\n",
    "from astropy.time import Time, TimeDelta\n",
    "\n",
    "from sunpy import timeseries as ts\n",
    "from sunpy.net import attrs as a\n",
    "from sunpy.net import hek\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import goesxrs_temp as gtem #from Ian's functions\n",
    "import Onsets_temp as onsets #from my fucntions\n",
    "\n",
    "# import warnings #Need to remove for future\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", module=\"sunpy.timeseries.timeseriesbase\")\n",
    "\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "\n",
    "# Just setup plot fonts\n",
    "plt.rcParams.update({'font.size': 18,'font.family':\"sans-serif\",\\\n",
    "                         'font.sans-serif':\"Arial\",'mathtext.default':\"regular\", 'axes.linewidth' :2})\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = r\"C:\\Users\\William\\Documents\\University\\MastersProj\\Data\\Paper_Flares\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Looping through all flares in directory to fetch onset info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------\n",
      "N/A Flare With Start Time: 2010-11-05 13:05:00\n",
      "Background Start: 2010-11-05 13:06:40, Background End: 2010-11-05 13:07:40\n",
      "Onset start: 2010-11-05 13:07:40, 1/4 Onset End: 2010-11-05 13:13:05.127750\n",
      "\n",
      "Onset Temperature: 9.199024553213112\n",
      "Upper bound:9.730312149320683; (+0.5312875961075711)\n",
      "lower bound: 8.633567244988646; (-0.5654573082244667)\n",
      "------------------------------\n",
      "N/A Flare With Start Time: 2011-02-14 01:31:00\n",
      "Background Start: 2011-02-14 01:31:10, Background End: 2011-02-14 01:32:10\n",
      "Onset start: 2011-02-14 01:32:22.791000, 1/4 Onset End: 2011-02-14 01:33:24.876250\n",
      "\n",
      "Onset Temperature: 14.857210495281862\n",
      "Upper bound:15.438634250257595; (+0.5814237549757326)\n",
      "lower bound: 14.260958355274079; (-0.5962521400077829)\n",
      "------------------------------\n",
      "N/A Flare With Start Time: 2012-05-14 13:33:00\n",
      "Background Start: 2012-05-14 13:34:10, Background End: 2012-05-14 13:35:10\n",
      "Onset start: 2012-05-14 13:35:11.311000, 1/4 Onset End: 2012-05-14 13:35:58.967000\n",
      "\n",
      "Onset Temperature: 8.739752994893989\n",
      "Upper bound:9.691492588017681; (+0.9517395931236923)\n",
      "lower bound: 7.6717940438646846; (-1.0679589510293042)\n",
      "------------------------------\n",
      "N/A Flare With Start Time: 2014-01-07 10:07:00\n",
      "Background Start: 2014-01-07 10:07:40, Background End: 2014-01-07 10:08:40\n",
      "Onset start: 2014-01-07 10:08:51.451000, 1/4 Onset End: 2014-01-07 10:09:47.373750\n",
      "\n",
      "Onset Temperature: 9.776396492583913\n",
      "Upper bound:10.383949256147265; (+0.6075527635633513)\n",
      "lower bound: 9.119377213161783; (-0.6570192794221299)\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n"
     ]
    }
   ],
   "source": [
    "#Looping through all goes-15 flares in directory\n",
    "flares_df = pd.read_csv('Background Timings.csv')\n",
    "# flare_data = pd.DataFrame(columns = [])\n",
    "\n",
    "onset_length = 60 #Time of fixed onset in seconds\n",
    "\n",
    "for index, row in flares_df.iterrows():\n",
    "     try:\n",
    "          #Locate Flare from downloaded files\n",
    "          fl_hek_start = row['Start Time']\n",
    "          fl_hek_end = row['End Time']\n",
    "          fl_class = \"N/A\"\n",
    "          flare_time = a.Time(Time(fl_hek_start, scale='utc').iso, Time(fl_hek_end,scale='utc').iso)\n",
    "          day_string = flare_time.start.datetime.date().strftime(\"%Y%m%d\")\n",
    "          fl_path = os.path.join(data_dir,f'*g15*{day_string}*.nc')\n",
    "\n",
    "          if Time(fl_hek_start).datetime.time() <= time(hour=1, minute=0, second=0):\n",
    "               day_str_prev = (Time(fl_hek_start) - TimeDelta(1, format = 'jd')).datetime.date().strftime(\"%Y%m%d\")\n",
    "               fl_path_prev = os.path.join(data_dir,f'*g15*{day_str_prev}*.nc')\n",
    "               g15 = ts.TimeSeries([fl_path, fl_path_prev],concatenate=True)\n",
    "          elif Time(fl_hek_start).datetime.time() >= time(hour=23, minute=0, second=0):\n",
    "               day_str_post = (Time(fl_hek_start)+ TimeDelta(1, format = 'jd')).datetime.date().strftime(\"%Y%m%d\")\n",
    "               fl_path_post = os.path.join(data_dir,f'*g15*{day_str_post}*.nc')\n",
    "               g15 = ts.TimeSeries([fl_path, fl_path_post],concatenate=True)\n",
    "          else:  \n",
    "               g15 = ts.TimeSeries(fl_path, concatenate=True)\n",
    "\n",
    "     # Load flare into time series\n",
    "          g_tims = g15.index\n",
    "          g_short = g15.quantity(\"xrsa\").value \n",
    "          g_long = g15.quantity(\"xrsb\").value\n",
    "          g_short_flag = g15.quantity(\"xrsa_quality\").value \n",
    "          g_long_flag = g15.quantity(\"xrsb_quality\").value \n",
    "\n",
    "     # Create a Boolean mask where onset flag is equal to 1\n",
    "          mask_long = g_long_flag == 1\n",
    "          mask_short = g_short_flag == 1\n",
    "\n",
    "          # Set the corresponding values in g_short and g_long to NaN\n",
    "          g_short[mask_short] = np.nan\n",
    "          g_long[mask_long] = np.nan\n",
    "          df_long = pd.Series(g_long, index = pd.DatetimeIndex(g15.index))\n",
    "          df_short = pd.Series(g_short, index = pd.DatetimeIndex(g15.index))\n",
    "\n",
    "     # Printing for terminal\n",
    "          print(\"------------------------------\")\n",
    "          print(f\"{fl_class} Flare With Start Time: {fl_hek_start}\")\n",
    "\n",
    "     #Calculating background of non-backsubbed peak\n",
    "          bcktrunc_18=df_long.truncate(flare_time.start.iso,flare_time.end.iso)\n",
    "          old_peak = bcktrunc_18.idxmax()\n",
    "\n",
    "     # Background Time Calculation Function\n",
    "          #bckt, bcktrunc_054, bcktrunc_18, bckdata = background_2(df_short, df_long, start_time = Time(row['Start Time']), peak_time = Time(row['Peak Time']))\n",
    "          bck_startt, bck_endt, smooth_bck, bck_tr_ts, bck_tr_vars, bck_flag = onsets.background(df_short, df_long, start_time = Time(fl_hek_start), peak_time = old_peak)\n",
    "          srch_time=a.Time(Time(bck_startt, scale='utc').iso, Time(bck_endt,scale='utc').iso)\n",
    "          trunc_054 = df_short.truncate(srch_time.start.iso,srch_time.end.iso)\n",
    "          trunc_18 = df_long.truncate(srch_time.start.iso,srch_time.end.iso)\n",
    "\n",
    "          print(f\"Background Start: {bck_startt}, Background End: {bck_endt}\")\n",
    "\n",
    "          bck_t = trunc_054.index\n",
    "          bck_short = np.mean(trunc_054)\n",
    "          bck_short_std = np.std(trunc_054)\n",
    "          bck_long = np.mean(trunc_18)\n",
    "          bck_long_std = np.std(trunc_054)\n",
    "\n",
    "          # print(f\"long channel backg err: {bck_long_std}\")\n",
    "          # print(f\"short channel backg err: {bck_short_std}\")\n",
    "\n",
    "     # Background Subtracting the Data\n",
    "          short_backsub = df_short - bck_short\n",
    "          long_backsub = df_long - bck_long\n",
    "\n",
    "     # Calculating true peak time (using backsubbed long channel flux)\n",
    "          flare_long = long_backsub.truncate(flare_time.start.iso,flare_time.end.iso)\n",
    "          long_peakt = flare_long.idxmax()\n",
    "          flare_short = short_backsub.truncate(flare_time.start.iso,flare_time.end.iso)\n",
    "          short_peakt = flare_short.idxmax()\n",
    "\n",
    "          long_peakfl_18 = flare_long[long_peakt]\n",
    "          long_peakfl_054 = flare_short[long_peakt]\n",
    "          short_peakfl_18 = flare_long[short_peakt]\n",
    "          short_peakfl_054 = flare_short[short_peakt]\n",
    "\n",
    "          true_peak = long_peakt #Varaible change for old code\n",
    "\n",
    "     # Calculating the first acceptable point of onset using backsubbed long channel\n",
    "          srch_start = Time(bck_endt) - TimeDelta(60, format = 'sec')\n",
    "          trange_search = a.Time(srch_start, true_peak)\n",
    "          g_long_srch = long_backsub.truncate(trange_search.start.iso,trange_search.end.iso)\n",
    "\n",
    "          \n",
    "          indexes = g_long_srch[g_long_srch >= 1e-8].index\n",
    "          if len(indexes) == 0:\n",
    "               raise ValueError\n",
    "          else:\n",
    "               first_index = indexes[0]\n",
    "               \n",
    "     # Onset and TEM Calculations of different onset definitions \n",
    "     # Lots of inefficient calculating going on here, but it makes it clearer to track the variables\n",
    "\n",
    "          #onset_start = (Time(bck_endt) + (Time(true_peak)-Time(bck_endt))/8).datetime # Calculating onset start to be 1/8th into the Impulsive phase\n",
    "          if first_index > bck_endt:\n",
    "               onset_start = first_index\n",
    "          else:\n",
    "               onset_start = bck_endt\n",
    "\n",
    "          # Calcualting the time length of the each fractional onset interval, as well as the end time of that interval\n",
    "          endt_eighth = onsets.scaled_onset(bck_endt, true_peak, 8)\n",
    "          timedelta_eighth = pd.to_timedelta(endt_eighth - onset_start).total_seconds()\n",
    "          endt_sixth = onsets.scaled_onset(bck_endt, true_peak, 6)\n",
    "          timedelta_sixth = pd.to_timedelta(endt_sixth - onset_start).total_seconds()\n",
    "          endt_quarter = onsets.scaled_onset(bck_endt, true_peak, 4)\n",
    "          timedelta_quarter = pd.to_timedelta(endt_quarter - onset_start).total_seconds()\n",
    "          endt_third = onsets.scaled_onset(bck_endt, true_peak, 3)\n",
    "          timedelta_third = pd.to_timedelta(endt_third - onset_start).total_seconds()\n",
    "          endt_half = onsets.scaled_onset(bck_endt, true_peak, 2)\n",
    "          timedelta_half = pd.to_timedelta(endt_half - onset_start).total_seconds()\n",
    "          endt_twothirds = onsets.scaled_onset(bck_endt, true_peak, 3/2)\n",
    "          timedelta_twothirds = pd.to_timedelta(endt_twothirds - onset_start).total_seconds()\n",
    "          endt_threequart = onsets.scaled_onset(bck_endt, true_peak, 4/3)\n",
    "          timedelta_threequart= pd.to_timedelta(endt_threequart - onset_start).total_seconds() \n",
    "\n",
    "          print(f\"Onset start: {onset_start}, 1/4 Onset End: {endt_quarter}\")\n",
    "\n",
    "     # # Calculating errors [IMPLEMENTED TO FUNCTION NOW]\n",
    "     #      #First calculate the poisson error (sqrt of flux for each raw flux bin in the range)\n",
    "     #      quarter_trunc_054 = df_short.truncate(onset_start, endt_quarter)\n",
    "     #      quarter_trunc_18 = df_long.truncate(onset_start, endt_quarter)\n",
    "     #      quarter_trunc_054_bck = short_backsub.truncate(onset_start, endt_quarter)\n",
    "     #      quarter_trunc_18_bck = long_backsub.truncate(onset_start, endt_quarter)\n",
    "\n",
    "     #      counts_unc_054 = np.sqrt(quarter_trunc_054.values) #Uncertainty on the raw unsubbed flux measurement\n",
    "     #      counts_unc_18 = np.sqrt(quarter_trunc_18.values)\n",
    "     #      flux_unc_054 = np.sqrt(counts_unc_054**2 + bck_short_std) #Uncertainty on the backsubbed flux\n",
    "     #      flux_unc_18 = np.sqrt(counts_unc_18**2 + back_long_std)\n",
    "\n",
    "     #      weights_054 = 1/(flux_unc_054)**2\n",
    "     #      weights_18 = 1/(flux_unc_18)**2\n",
    "     #      weighted_mean_054 = np.sum(weights_054 * quarter_trunc_054_bck) / np.sum(weights_054)\n",
    "     #      weighted_mean_18 = np.sum(weights_18 * quarter_trunc_18_bck) / np.sum(weights_18)\n",
    "     #      err_054 = np.sqrt(np.sum(weights_054 * (quarter_trunc_054_bck - weighted_mean_054)**2) / (np.sum(weights_054) * (len(quarter_trunc_054_bck)) - 1))\n",
    "     #      err_18 = np.sqrt(np.sum(weights_18 * (quarter_trunc_18_bck - weighted_mean_18)**2) / (np.sum(weights_18) * (len(quarter_trunc_18_bck)) - 1))\n",
    "          \n",
    "     #      print(f\"Error on short channel weighted avg flux is: {err_054}; {(err_054/weighted_mean_054)*100}\")\n",
    "     #      print(f\"Error on long channel weighted avg flux is: {err_18}; {(err_18/weighted_mean_18)*100}\")\n",
    "\n",
    "     #      ratio = weighted_mean_054/weighted_mean_18\n",
    "     #      ratio_err = ratio * np.sqrt((err_054/weighted_mean_054)**2 + (err_18/weighted_mean_18)**2)\n",
    "     #      print(f'Error on the ratio is: {ratio_err}; {(ratio_err/ratio)*100}')\n",
    "\n",
    "     # Calculating the temp and em for the onset of each fractional onset interval\n",
    "          #inputs: short_raw, long_raw, short_backsub, long_backsub, onset_start, onset_end, bck_short_std, bck_long_std\n",
    "          #Outputs: tmk_onset, em_onset, tmk_upper, tmk_lower, em_upper, em_lower\n",
    "\n",
    "\n",
    "\n",
    "          # tmk_eighth, em_eighth = onsets.onset_tem(short_backsub, long_backsub, onset_start, endt_eighth)\n",
    "          # tmk_sixth, em_sixth = onsets.onset_tem(short_backsub, long_backsub, onset_start, endt_sixth)\n",
    "          tmk_quarter, em_quarter, tmk_upper_quarter, tmk_lower_quarter, em_upper_quarter, em_lower_quarter  = onsets.onset_tem(df_short, df_long, short_backsub, long_backsub, onset_start, endt_quarter, bck_short_std, bck_long_std)\n",
    "          # tmk_third, em_third = onsets.onset_tem(short_backsub, long_backsub, onset_start, endt_third)\n",
    "          # tmk_half, em_half = onsets.onset_tem(short_backsub, long_backsub, onset_start, endt_half)\n",
    "          # tmk_twothirds, em_twothirds = onsets.onset_tem(short_backsub, long_backsub, onset_start, endt_twothirds)\n",
    "          # tmk_threequart, em_threequart = onsets.onset_tem(short_backsub, long_backsub, onset_start, endt_threequart)\n",
    "\n",
    "          print(\"\")\n",
    "          print(f\"Onset Temperature: {tmk_quarter}\")\n",
    "          print(f'Upper bound:{tmk_upper_quarter}; (+{tmk_upper_quarter - tmk_quarter})')\n",
    "          print(f'lower bound: {tmk_lower_quarter}; (-{tmk_quarter - tmk_lower_quarter})')\n",
    "     \n",
    "     #Calculating error on TEM\n",
    "          # print(\"\")\n",
    "          # temp_err_red = tmk_quarter*(ratio_err/ratio)**2\n",
    "          # temp_err = tmk_quarter*np.sqrt((0.2)**2 + (ratio_err/ratio)**2)\n",
    "          # print(f\"Temp error is: {temp_err}; {(temp_err/tmk_quarter)*100}\")\n",
    "          # print(f\"Reduced temp err is: {temp_err_red}\")\n",
    "\n",
    "          # EM_err_red = em_quarter * (err_18/weighted_mean_18)**2\n",
    "          # EM_err = em_quarter * np.sqrt((0.2)**2 + (err_18/weighted_mean_18)**2)\n",
    "          # print(f\"EM error is: {EM_err}; {(EM_err/em_quarter)*100}\")\n",
    "          # print(f\"Reduced EM err is: {EM_err_red}\")\n",
    "          # print(\"\")\n",
    "\n",
    "\n",
    "     # # Using my 'fancy' method\n",
    "     #      onset_start_fancy, onset_end_fancy = onsets.onset_times(short_backsub, bck_endt, true_peak, 5*bck_short_std)\n",
    "     #      try:\n",
    "     #           timedelta_fancy = (onset_end_fancy - onset_start_fancy).total_seconds()\n",
    "     #      except:\n",
    "     #           timedelta_fancy = np.nan\n",
    "\n",
    "     #      trunc_054_tem_fancy = np.mean(short_backsub.truncate(onset_start_fancy, onset_end_fancy))\n",
    "     #      trunc_18_tem_fancy = np.mean(long_backsub.truncate(onset_start_fancy, onset_end_fancy))\n",
    "     #      tmk_mnfancy, em_mnfancy = gtem.get_tem(trunc_18_tem_fancy, trunc_054_tem_fancy)\n",
    "\n",
    "     # # Calculating temperature at flare peak (long channel)\n",
    "     #      trunc_054_tem_peakl = short_backsub[true_peak]\n",
    "     #      trunc_18_tem_peakl = long_backsub[true_peak]\n",
    "     #      tmk_peakl, em_peakl = gtem.get_tem(trunc_18_tem_peakl, trunc_054_tem_peakl)\n",
    "\n",
    "     # #Calculating temperature at flare peak (short_channel)\n",
    "     #      trunc_054_tem_peaks = short_backsub[short_peakt]\n",
    "     #      trunc_18_tem_peaks = long_backsub[short_peakt]\n",
    "     #      tmk_peaks, em_peaks = gtem.get_tem(trunc_18_tem_peaks, trunc_054_tem_peaks)\n",
    "\n",
    "     # # Calculating max flare temperature\n",
    "     #      trunc_054_tem_max = short_backsub.truncate(onset_end_fancy, Time(true_peak).datetime)\n",
    "     #      trunc_18_tem_max = long_backsub.truncate(onset_end_fancy, Time(true_peak).datetime)\n",
    "     #      tmk_maxx, em_maxx = gtem.get_tem(trunc_18_tem_max, trunc_054_tem_max)\n",
    "     \n",
    "     except ValueError: \n",
    "          print(\"error\")\n",
    "          # try:\n",
    "          #      tmk_max = np.nanmax(tmk_maxx)\n",
    "          #      tmk_max_index = np.idxmax(tmk_max)\n",
    "          #      em_max = em_maxx[tmk_max_index]\n",
    "          # except:\n",
    "          #      tmk_max = np.nan\n",
    "          #      em_max = np.nan\n",
    "\n",
    "#           flare_data = pd.concat([flare_data, pd.DataFrame({'Actual Peak Time': [true_peak], 'Peak Time NoSub': [old_peak], 'Peak Time': [true_peak], 'Peak Flux': [long_peakfl_18] ,'Background End Time': [bck_endt],\n",
    "#                                                             'Fancy Temp': [tmk_mnfancy],'Fancy Temp Nosub': [np.nan],'Fancy EM': [em_mnfancy], 'Fancy EM Nosub':[np.nan],'Fancy Tdelta': [timedelta_fancy], 'Peak Temp Long': [tmk_peakl], 'Peak EM Long': [em_peakl],'Peak Temp Short': [tmk_peaks], 'Peak EM Short': [em_peaks], 'Flare Max Temp': [tmk_max],\n",
    "#                                                             'Flare Max EM': [em_max],'Temp 1/8': [tmk_eighth], 'EM 1/8': [em_eighth],'Tdelta 1/8': [timedelta_eighth],'Temp 1/6': [tmk_sixth], 'EM 1/6': [em_sixth],'Tdelta 1/6': [timedelta_sixth],'Temp 1/4': [tmk_quarter],\n",
    "#                                                             'EM 1/4': [em_quarter], 'Tdelta 1/4': [timedelta_quarter],'Temp 1/3': [tmk_third], 'EM 1/3': [em_third],'Tdelta 1/3':[timedelta_third],'Temp 1/2': [tmk_half], 'EM 1/2': [em_half],'Tdelta 1/2': [timedelta_half],'Temp 2/3': [tmk_twothirds], \n",
    "#                                                             'EM 2/3': [em_twothirds],'Tdelta 2/3': [timedelta_twothirds], 'Temp 3/4': [tmk_threequart], 'EM 3/4': [em_threequart], 'Tdelta 3/4': [timedelta_threequart], 'Background Flag': [bck_flag], 'Onset Flag': [np.nan]})], ignore_index=True)\n",
    "\n",
    "\n",
    "#      except ValueError:\n",
    "#           print(\"---------------------------------------\")\n",
    "#           print(\"Unexpected error:\", sys.exc_info()[0])\n",
    "#           print(\"ERROR, NAN ROW PLACED\")\n",
    "#           print(\"---------------------------------------\")\n",
    "#           flare_data = pd.concat([flare_data, pd.DataFrame({'Actual Peak Time': [np.nan], 'Peak Time NoSub': [np.nan], 'Peak Time': [np.nan],'Peak Flux': [np.nan],'Background End Time': [np.nan],\n",
    "#                                                             'Fancy Temp': [np.nan], 'Fancy Temp Nosub':[np.nan],'Fancy EM': [np.nan],'Fancy EM Nosub': [np.nan],'Fancy Tdelta': [np.nan], 'Peak Temp Long': [np.nan], 'Peak EM Long': [np.nan],'Peak Temp Short': [np.nan], 'Peak EM Short': [np.nan], 'Flare Max Temp': [np.nan],\n",
    "#                                                             'Flare Max EM': [np.nan],'Temp 1/8': [np.nan], 'EM 1/8': [np.nan],'Tdelta 1/8': [np.nan],'Temp 1/6': [np.nan], 'EM 1/6': [np.nan],'Tdelta 1/6': [np.nan],'Temp 1/4': [np.nan],\n",
    "#                                                             'EM 1/4': [np.nan], 'Tdelta 1/4': [np.nan],'Temp 1/3': [np.nan], 'EM 1/3': [np.nan],'Tdelta 1/3':[np.nan],'Temp 1/2': [np.nan], 'EM 1/2': [np.nan],'Tdelta 1/2': [np.nan],'Temp 2/3': [np.nan], \n",
    "#                                                             'EM 2/3': [np.nan],'Tdelta 2/3': [np.nan], 'Temp 3/4': [np.nan], 'EM 3/4': [np.nan], 'Tdelta 3/4': [np.nan], 'Background Flag': [np.nan], 'Onset Flag': [np.nan]})], ignore_index=True)\n",
    "#           continue\n",
    "\n",
    "# if len(flare_data) == len(flares_df):\n",
    "#      full_data = pd.concat([flares_df, flare_data],axis = 1)\n",
    "# else:\n",
    "#      print(\"ERROR, data lengths do not match\")\n",
    "\n",
    "# full_data.to_csv(\"2016_Flare_Data_Processed_9_avg.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hot_onsets",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "62a2b8ad637c63baaf45e2cce9d4a159ca450ba6124aa82e6d4dc02e44296dbd"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
